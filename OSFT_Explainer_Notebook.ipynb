{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b665244c",
   "metadata": {},
   "source": [
    "# OSFT (Orthogonal Subspace Fine-Tuning) Tutorial Notebook\n",
    "\n",
    "## by Frank La Vigne\n",
    "\n",
    "Alright, here’s the deal. OSFT is all about teaching your model new tricks without it forgetting the old ones. If you’ve ever fine-tuned a model and watched it suddenly get dumber at stuff it used to know — that’s catastrophic forgetting. OSFT is how we fight back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0dd6f5",
   "metadata": {},
   "source": [
    "## Key Ideas\n",
    "- Break down weight matrices with SVD (like putting on X-Ray specs for your model).\n",
    "- Spot which directions in parameter space are pulling their weight vs. which ones are idle.\n",
    "- Keep updates out of the “critical” directions and funnel them into the unused space.\n",
    "- End result: your model learns new stuff without trashing the old knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab839c-0c5a-43c1-b086-0685957c97ae",
   "metadata": {},
   "source": [
    "# 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215d4f2-7a48-401e-87d0-52507527342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0599f85",
   "metadata": {},
   "source": [
    "# 2. Warm-Up with NumPy: Seeing the Subspace\n",
    "\n",
    "Before we touch PyTorch, let’s warm up with NumPy. We’ll:\n",
    "1. Take a toy weight matrix.\n",
    "2. Run SVD to split it into important vs. not-so-important directions.\n",
    "3. Project a gradient update into the “safe” zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f514d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A toy weight matrix (e.g. from a linear layer)\n",
    "W = np.array([[2.0, 0.5, 0.0],\n",
    "              [0.0, 1.5, 0.1],\n",
    "              [0.0, 0.0, 0.2]])\n",
    "print(\"Original weight matrix W:\\n\", W)\n",
    "\n",
    "# Perform SVD decomposition\n",
    "U, S, Vt = np.linalg.svd(W)\n",
    "print(\"Singular values:\", S)\n",
    "\n",
    "# Define high-rank vs low-rank subspaces\n",
    "rank_cutoff = 1  # keep top-1 singular vector as important\n",
    "U_high = U[:, :rank_cutoff]\n",
    "V_high = Vt[:rank_cutoff, :].T\n",
    "\n",
    "# Any gradient update\n",
    "grad = np.array([[0.1, -0.2, 0.05],\n",
    "                 [0.05, 0.1, -0.1],\n",
    "                 [-0.2, 0.0, 0.2]])\n",
    "\n",
    "# Project gradient onto low-rank subspace (orthogonal to U_high, V_high)\n",
    "proj = grad - U_high @ (U_high.T @ grad @ V_high) @ V_high.T\n",
    "\n",
    "print(\"Original gradient:\\n\", grad)\n",
    "print(\"Projected gradient (OSFT):\\n\", proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea427be",
   "metadata": {},
   "source": [
    "Look at that — the projected gradient steers clear of the critical directions. That’s OSFT on training wheels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e7a11",
   "metadata": {},
   "source": [
    "# 3. How the Training Loop Looks (Pseudo-code)\n",
    "\n",
    "Here’s the play-by-play in pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for each training step:\n",
    "    for each layer l in model:\n",
    "        W = layer.weight\n",
    "        U, S, Vt = svd(W)\n",
    "        r = retention_ratio(layer)  # based on importance\n",
    "        U_high = U[:, :r]\n",
    "        V_high = Vt[:r, :].T\n",
    "\n",
    "        grad = compute_gradient(layer)\n",
    "        grad_proj = grad - U_high @ (U_high.T @ grad @ V_high) @ V_high.T\n",
    "\n",
    "        apply_update(layer, grad_proj)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41fe3c0",
   "metadata": {},
   "source": [
    "# 4. Hands-on PyTorch Demo\n",
    "\n",
    "Time to level up. Let’s try this on a small PyTorch model. We’ll train it on dummy data, then apply OSFT-style projection to the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a small model\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(3, 3, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SmallNet()\n",
    "\n",
    "# Dummy data\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 3)\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Forward + backward\n",
    "loss_fn = nn.MSELoss()\n",
    "output = model(x)\n",
    "loss = loss_fn(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Inspect original gradient\n",
    "grad = model.fc.weight.grad.detach().numpy()\n",
    "print(\"Original gradient:\\n\", grad)\n",
    "\n",
    "# Perform SVD on weights\n",
    "W = model.fc.weight.detach().numpy()\n",
    "U, S, Vt = np.linalg.svd(W)\n",
    "\n",
    "# Keep top-1 singular vector as \"critical\"\n",
    "rank_cutoff = 1\n",
    "U_high = U[:, :rank_cutoff]\n",
    "V_high = Vt[:rank_cutoff, :].T\n",
    "\n",
    "# Project PyTorch gradient into low-rank subspace\n",
    "grad_proj = grad - U_high @ (U_high.T @ grad @ V_high) @ V_high.T\n",
    "\n",
    "# Replace gradient in model with projected version\n",
    "model.fc.weight.grad = torch.from_numpy(grad_proj).float()\n",
    "\n",
    "# Apply update\n",
    "opt.step()\n",
    "\n",
    "print(\"Updated weights (after OSFT-style projection):\\n\", model.fc.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ddf1f",
   "metadata": {},
   "source": [
    "There it is. The model took a step — but only in the directions we allowed. That’s how OSFT threads the needle: new learning without wrecking the old foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca8592",
   "metadata": {},
   "source": [
    "# 5. Why This Matters\n",
    "- **Text classification sequences:** Keeps performance steady across 5, 10, 15+ tasks.\n",
    "- **TRACE benchmark:** Boosted LLaMA-2-7B’s accuracy by ~7 points over O-LoRA.\n",
    "- **Enterprise bots:** Add new product knowledge without erasing FAQs from last year.\n",
    "- **Medical/legal models:** Stay current with new research, but don’t forget the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03c5e9",
   "metadata": {},
   "source": [
    "# 6. Wrap-Up\n",
    "OSFT in a nutshell:\n",
    "- SVD shows us the model’s “critical” vs. “safe” directions.\n",
    "- We project updates into the safe zone.\n",
    "- The model grows new skills while keeping the old ones sharp.\n",
    "\n",
    "Think of it like renovating a house. OSFT adds a new room without tearing down the walls that are already holding up the place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
