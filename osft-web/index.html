<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Orthogonal Fine-Tuning Interactive Guide</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/11.11.0/math.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <h1>🔄 Orthogonal Fine-Tuning</h1>
            <div class="nav-links">
                <a href="#intro">Introduction</a>
                <a href="#oft">OFT Basics</a>
                <a href="#osft">OSFT</a>
                <a href="#visualizations">Visualizations</a>
                <a href="#comparison">Comparison</a>
            </div>
        </div>
    </nav>

    <main class="container">
        <!-- Hero Section -->
        <section id="intro" class="hero">
            <h2>Understanding Orthogonal Fine-Tuning</h2>
            <p class="lead">Master parameter-efficient fine-tuning techniques that preserve learned knowledge while adapting to new tasks.</p>
            <div class="hero-stats">
                <div class="stat">
                    <span class="stat-number">90%+</span>
                    <span class="stat-label">Parameter Reduction</span>
                </div>
                <div class="stat">
                    <span class="stat-number">Stable</span>
                    <span class="stat-label">Training</span>
                </div>
                <div class="stat">
                    <span class="stat-number">Zero</span>
                    <span class="stat-label">Forgetting</span>
                </div>
            </div>
        </section>

        <!-- OFT Introduction -->
        <section id="oft" class="content-section">
            <h2>🚪 Through the Looking Glass: What is OFT?</h2>
            <div class="two-column">
                <div>
                    <p>Just as Alice stepped through the looking glass and found a world that was familiar yet rearranged, OFT adapts pre-trained models by <strong>rotating</strong> their learned representations rather than distorting them.</p>

                    <p>OFT applies <strong>orthogonal transformations</strong> to weight matrices—think of it as spinning the looking glass to see your model's knowledge from a new perspective, without warping the reflection.</p>

                    <h3>🔮 The Magic Mirror Properties</h3>
                    <ul>
                        <li><strong>Preserves distances:</strong> Alice stays the same size (||Qx|| = ||x||)</li>
                        <li><strong>Preserves angles:</strong> The Cheshire Cat's grin keeps its shape</li>
                        <li><strong>Identity when transposed:</strong> The mirror reflects perfectly (Q<sup>T</sup>Q = I)</li>
                        <li><strong>Represents rotations:</strong> Turn the mirror, don't bend it</li>
                    </ul>

                    <div class="formula-box">
                        <h4>🎭 The Looking Glass Formula</h4>
                        <code>W' = W × R</code>
                        <p>Where R is the orthogonal mirror that rotates (not reshapes) your model's knowledge</p>
                    </div>
                </div>
                <div>
                    <div class="interactive-demo" id="orthogonal-demo">
                        <canvas id="orthogonalCanvas" width="400" height="400"></canvas>
                        <div class="controls">
                            <label>
                                🔄 Rotation Angle: <span id="angleValue">45°</span>
                                <input type="range" id="rotationAngle" min="0" max="360" value="45">
                            </label>
                            <button id="resetRotation">Reset Mirror</button>
                        </div>
                        <p class="rabbit-hole-note" style="margin-top: 1rem;">🪞 <em>Watch how the points rotate together, like Wonderland itself spinning—everything moves, but relationships stay true.</em></p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Orthogonal Matrix Verification -->
        <section class="content-section">
            <h2>🎴 The Queen's Matrix Garden</h2>
            <p>In the Queen of Hearts' garden, every card has its place. An orthogonal matrix is like the Queen's decree—it can rearrange the cards (rotate them), but it must preserve their ranks and suits (mathematical properties).</p>

            <div class="matrix-explorer">
                <div class="matrix-display">
                    <h3>🌹 Matrix Q</h3>
                    <div id="matrixQ" class="matrix"></div>
                    <p style="font-size: 0.85rem; margin-top: 0.5rem;">The Queen's arrangement</p>
                </div>
                <div class="matrix-display">
                    <h3>👑 Q<sup>T</sup> × Q</h3>
                    <div id="matrixQTQ" class="matrix"></div>
                    <p style="font-size: 0.85rem; margin-top: 0.5rem;">Should be the Identity (all ones on diagonal)</p>
                </div>
                <div class="matrix-display">
                    <h3>✨ Royal Properties</h3>
                    <div id="matrixProperties" class="properties"></div>
                </div>
            </div>
            <button id="generateMatrix">🎲 Shuffle the Cards (Generate New Matrix)</button>

            <p class="rabbit-hole-note" style="margin-top: 1.5rem;">♥️ <em>"Off with their heads!" cries the Queen—but an orthogonal matrix is merciful. It moves the cards without changing their essence. Q<sup>T</sup>Q always equals Identity, proving the transformation is pure rotation.</em></p>
        </section>

        <!-- Hyperspherical Energy -->
        <section class="content-section bg-light">
            <h2>Hyperspherical Energy Preservation</h2>
            <p>A key advantage of OFT: preserving the <strong>hyperspherical energy</strong> of features—the geometric relationships between neuron activations.</p>

            <div class="energy-comparison">
                <canvas id="energyCanvas" width="800" height="400"></canvas>
                <div class="energy-stats" id="energyStats"></div>
            </div>

            <div class="explanation">
                <h3>🎩 Why This Matters: The Mad Hatter's Tea Party Problem</h3>
                <p>Imagine you're at the Mad Hatter's tea party, and everyone's seat represents a learned feature in your model. Traditional fine-tuning is like the Hatter shouting "Move down!" - everyone shifts chaotically, and suddenly the March Hare is sitting where the Dormouse should be. The entire seating arrangement (your model's learned relationships) gets scrambled.</p>

                <p><strong>OFT is different.</strong> It's like rotating the entire table instead of moving individual seats. Everyone maintains their relative positions - if Alice was between the Hatter and the Hare before, she still is after the rotation. The <em>relationships</em> stay intact.</p>

                <p>In technical terms: By preserving angular relationships (hyperspherical energy), OFT maintains the semantic structure learned during pre-training. Features that were similar before fine-tuning remain similar after. Features that were orthogonal (independent) stay orthogonal. This leads to:</p>

                <ul>
                    <li><strong>More stable training:</strong> No chaotic seat-swapping means gradients behave predictably</li>
                    <li><strong>Better generalization:</strong> The model's core understanding remains intact</li>
                    <li><strong>Preserved knowledge:</strong> Like keeping the tea party's social dynamics while just moving to a different garden</li>
                </ul>

                <p class="rabbit-hole-note">💫 <em>Down here in Wonderland, we don't break what already works - we just rotate it to see it from a new angle.</em></p>
            </div>
        </section>

        <!-- OSFT Introduction -->
        <section id="osft" class="content-section">
            <h2>🗝️ The Deeper Rabbit Hole: OSFT</h2>
            <div class="osft-intro">
                <p class="lead">Remember when Alice found the tiny golden key? OSFT is like having a map showing which doors are vital to Wonderland's structure (critical subspaces) and which are just decoration (safe directions for updates).</p>

                <p>OSFT takes OFT further by using <strong>SVD decomposition</strong>—think of it as the Caterpillar's magnifying glass that reveals which parts of the garden are essential and which can be replanted.</p>

                <div class="problem-solution">
                    <div class="problem">
                        <h3>🍄 The Problem: Eating the Wrong Mushroom</h3>
                        <p>Standard fine-tuning is like Alice eating random mushrooms—she might grow taller (learn new tasks) but forget how to get back to normal size (catastrophic forgetting). The model loses what it knew while learning something new.</p>
                    </div>
                    <div class="solution">
                        <h3>🎯 The Solution: The Caterpillar's Wisdom</h3>
                        <p>"One side makes you taller, the other makes you shorter." OSFT identifies safe directions (one side of the mushroom) where updates won't harm critical knowledge. We only eat from the safe side!</p>
                    </div>
                </div>

                <p class="rabbit-hole-note" style="margin-top: 1.5rem;">🐛 <em>"Who are YOU?" said the Caterpillar. OSFT answers: "I'm the same model, just viewing from a different angle—my core identity (critical subspace) intact."</em></p>
            </div>
        </section>

        <!-- SVD Visualization -->
        <section class="content-section">
            <h2>🔍 The Cheshire Cat's Grin: SVD Decomposition</h2>
            <p>"Well! I've often seen a cat without a grin," thought Alice, "but a grin without a cat! It's the most curious thing!"</p>

            <p>SVD is like the Cheshire Cat revealing which parts of itself are essential (the grin—critical directions) and which can vanish (the body—safe for modification). It decomposes weight matrices to show us what must stay and what can change.</p>

            <div class="svd-explorer">
                <div class="svd-visual">
                    <canvas id="svdCanvas" width="600" height="400"></canvas>
                </div>
                <div class="svd-controls">
                    <h3>😸 Cheshire Controls</h3>
                    <label>
                        Rank Cutoff (What Stays Visible): <span id="rankValue">1</span>
                        <input type="range" id="rankSlider" min="1" max="3" value="1">
                    </label>
                    <div class="singular-values" id="singularValues"></div>
                    <button id="regenerateSVD">🎭 Fade and Reappear</button>
                </div>
            </div>

            <p class="rabbit-hole-note" style="margin-top: 1.5rem;">😺 <em>The larger the singular value (σ), the more "grin-like" it is—essential to the model's identity. Smaller values? Those parts can fade away without losing who the cat really is.</em></p>

            <div class="code-example">
                <h3>🧪 The White Rabbit's Recipe</h3>
                <pre><code class="language-python">import numpy as np

# The Cheshire Cat reveals its structure
U, S, Vt = np.linalg.svd(W)

# Identify the grin (critical subspace)
rank_cutoff = 1
U_high = U[:, :rank_cutoff]  # The essential smile
V_high = Vt[:rank_cutoff, :].T

# Project gradient to where the body was (safe subspace)
grad_proj = grad - U_high @ (U_high.T @ grad @ V_high) @ V_high.T
# Now we update only the parts that can safely vanish!</code></pre>
            </div>
        </section>

        <!-- Gradient Projection Demo -->
        <section class="content-section bg-light">
            <h2>🧭 Finding Your Way: Gradient Projection</h2>
            <p>"Would you tell me, please, which way I ought to go from here?" asked Alice.</p>
            <p>"That depends a good deal on where you want to get to," said the Cat.</p>

            <p>OSFT is like having the Cheshire Cat guide Alice's path. The <strong>critical direction</strong> (shown in red) is the path that must not be disturbed—it's the way back home. OSFT projects gradients to walk orthogonal to this path, exploring Wonderland without getting lost.</p>

            <div class="projection-demo">
                <canvas id="projectionCanvas" width="600" height="500"></canvas>
                <div class="projection-controls">
                    <h3>🎯 Choose Alice's Direction</h3>
                    <label>
                        Gradient X: <span id="gradXValue">0.5</span>
                        <input type="range" id="gradX" min="-2" max="2" step="0.1" value="0.5">
                    </label>
                    <label>
                        Gradient Y: <span id="gradYValue">0.8</span>
                        <input type="range" id="gradY" min="-2" max="2" step="0.1" value="0.8">
                    </label>
                    <div class="projection-info" id="projectionInfo"></div>
                </div>
            </div>

            <p class="rabbit-hole-note" style="margin-top: 1.5rem;">🐱 <em>"We're all mad here," said the Cat. "But OSFT's madness has method—it steers clear of critical paths while still moving forward. The green arrow shows where we actually step, safely away from the red forbidden direction."</em></p>
        </section>

        <!-- OFT vs LoRA Comparison -->
        <section id="comparison" class="content-section">
            <h2>OFT vs LoRA: Parameter Efficiency</h2>

            <div class="comparison-grid">
                <div class="method-card">
                    <h3>🔄 OFT</h3>
                    <p>Orthogonal transformations preserve geometric structure</p>
                    <ul>
                        <li>Stable training</li>
                        <li>Energy preservation</li>
                        <li>Better for domain adaptation</li>
                    </ul>
                </div>
                <div class="method-card">
                    <h3>📊 LoRA</h3>
                    <p>Low-rank decomposition for efficient updates</p>
                    <ul>
                        <li>Very parameter efficient</li>
                        <li>Additive updates</li>
                        <li>Popular for LLMs</li>
                    </ul>
                </div>
                <div class="method-card">
                    <h3>🎯 OSFT</h3>
                    <p>Combines orthogonality with subspace protection</p>
                    <ul>
                        <li>Zero catastrophic forgetting</li>
                        <li>Continual learning</li>
                        <li>Multi-task scenarios</li>
                    </ul>
                </div>
            </div>

            <div class="param-calculator">
                <h3>🧮 The Caterpillar's Abacus: Parameter Efficiency Calculator</h3>

                <p style="margin-bottom: 1.5rem;">"Who are YOU?" asks the Caterpillar. In fine-tuning, the answer depends on how many parameters you need. Let's see how small we can make the changes while keeping the same transformative power!</p>

                <div class="calculator-inputs">
                    <label>
                        Input Features:
                        <input type="number" id="inputFeatures" value="512" min="1">
                    </label>
                    <label>
                        Output Features:
                        <input type="number" id="outputFeatures" value="256" min="1">
                    </label>
                    <label>
                        Rank:
                        <input type="number" id="rank" value="16" min="1">
                    </label>
                </div>
                <div class="calculator-results" id="calculatorResults"></div>

                <div class="explanation" style="margin-top: 1.5rem;">
                    <h4>📏 Understanding the Numbers</h4>
                    <p><strong>Full Fine-Tuning</strong> is like rewriting the entire book of Wonderland—every parameter (word) can change. For a layer with 512 inputs and 256 outputs, that's <strong>131,072 parameters</strong>!</p>

                    <p><strong>OFT</strong> is like rotating the pages—you only need to specify the angle of rotation. With rank 16, that's just <strong>256 parameters</strong> (16 × 16), a 99.8% reduction! The Caterpillar would approve of such efficient transformation.</p>

                    <p><strong>LoRA</strong> takes a different approach—it's like adding margin notes instead of rewriting. It needs <strong>rank × (input + output)</strong> parameters. Still efficient, but not quite as parsimonious as OFT's rotation.</p>

                    <p class="rabbit-hole-note" style="margin-top: 1rem;">🐛 <em>"One side makes you larger, the other makes you smaller"—but with OFT, you can grow your model's capabilities while shrinking the parameter count. That's the magic of orthogonal transformations!</em></p>
                </div>
            </div>
        </section>

        <!-- Use Cases -->
        <section class="content-section">
            <h2>Real-World Applications</h2>

            <div class="use-cases">
                <div class="use-case">
                    <div class="icon">🎯</div>
                    <h3>Domain Adaptation</h3>
                    <p>Fine-tune models to related domains without losing general knowledge</p>
                    <span class="badge">OFT</span>
                </div>

                <div class="use-case">
                    <div class="icon">🔬</div>
                    <h3>Few-Shot Learning</h3>
                    <p>Adapt with limited data while maintaining robustness</p>
                    <span class="badge">OFT</span>
                </div>

                <div class="use-case">
                    <div class="icon">📚</div>
                    <h3>Continual Learning</h3>
                    <p>Learn new tasks sequentially without forgetting previous ones</p>
                    <span class="badge">OSFT</span>
                </div>

                <div class="use-case">
                    <div class="icon">🤖</div>
                    <h3>Enterprise Chatbots</h3>
                    <p>Update with new information while preserving core capabilities</p>
                    <span class="badge">OSFT</span>
                </div>

                <div class="use-case">
                    <div class="icon">⚕️</div>
                    <h3>Medical AI</h3>
                    <p>Stay current with research without forgetting fundamentals</p>
                    <span class="badge">OSFT</span>
                </div>

                <div class="use-case">
                    <div class="icon">⚖️</div>
                    <h3>Legal Models</h3>
                    <p>Incorporate new regulations while maintaining legal knowledge</p>
                    <span class="badge">OSFT</span>
                </div>
            </div>
        </section>

        <!-- Interactive Training Demo -->
        <section id="visualizations" class="content-section bg-light">
            <h2>Training Dynamics Comparison</h2>
            <p>Compare how different fine-tuning methods affect model performance over time.</p>

            <div class="training-viz">
                <canvas id="trainingCanvas" width="800" height="400"></canvas>
                <div class="training-controls">
                    <button id="startTraining">Start Training Simulation</button>
                    <button id="resetTraining">Reset</button>
                    <div class="legend">
                        <div><span class="legend-color oft"></span> OFT</div>
                        <div><span class="legend-color lora"></span> LoRA</div>
                        <div><span class="legend-color osft"></span> OSFT</div>
                        <div><span class="legend-color full"></span> Full Fine-Tuning</div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Key Takeaways -->
        <section class="content-section">
            <h2>🎪 Lessons from Wonderland: Key Takeaways</h2>

            <div class="takeaways">
                <div class="takeaway">
                    <h3>🪞 The Looking Glass Principle (OFT)</h3>
                    <p>Like spinning a mirror rather than cracking it, orthogonal transformations rotate the feature space without distortion. Alice stays Alice, just viewed from a new angle—geometric relationships preserved perfectly.</p>
                </div>

                <div class="takeaway">
                    <h3>😸 The Cheshire Grin Strategy (OSFT)</h3>
                    <p>Keep the grin (critical directions), let the body fade (safe subspace). OSFT identifies what's essential to your model's identity and protects it, enabling continual learning without losing who you are—no catastrophic forgetting!</p>
                </div>

                <div class="takeaway">
                    <h3>🎩 The Mad Hatter's Efficiency</h3>
                    <p>"Why is a raven like a writing desk?" Because both OFT and OSFT achieve 90%+ parameter reduction! Fewer parameters to tune, but the tea party keeps its charm. More efficient than full fine-tuning, just as effective.</p>
                </div>

                <div class="takeaway">
                    <h3>🌹 The Queen's Decree (Stability)</h3>
                    <p>"All ways are MY ways!" Orthogonal constraints keep gradients well-behaved—no explosion, no vanishing. The Queen's rules (mathematical constraints) ensure orderly training. Off with gradient chaos!</p>
                </div>
            </div>

            <p class="rabbit-hole-note" style="margin-top: 2rem; text-align: center; font-size: 1.1rem;">🐇 <em>"Curiouser and curiouser!" cried Alice. And indeed—the deeper you go down this orthogonal rabbit hole, the more elegant the mathematics becomes.</em></p>
        </section>

        <!-- Resources -->
        <section class="content-section">
            <h2>📚 The White Rabbit's Library</h2>
            <p>"Oh dear! Oh dear! I shall be too late!" The White Rabbit is always in a hurry to learn more. Here are the scrolls and tomes for your continued journey:</p>

            <div class="resources">
                <h3>🔬 Research Scrolls (Papers)</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2306.07280" target="_blank">📜 Orthogonal Fine-Tuning (OFT) - The Original Map</a></li>
                    <li><a href="#">📜 OSFT: The Deeper Rabbit Hole Explained</a></li>
                    <li><a href="#">📜 Parameter-Efficient Methods: A Wonderland Survey</a></li>
                </ul>

                <h3>⚙️ The Tinker's Workshop (Implementation)</h3>
                <ul>
                    <li><a href="#">🔧 PyTorch OFT: Build Your Own Looking Glass</a></li>
                    <li><a href="#">🔧 Hugging Face PEFT: The Queen's Toolkit</a></li>
                    <li><a href="#">🔧 Example Notebooks: The Caterpillar's Recipes</a></li>
                </ul>
            </div>

            <p class="rabbit-hole-note" style="margin-top: 2rem;">📖 <em>"Begin at the beginning," the King said, very gravely, "and go on till you come to the end: then stop." But in Wonderland (and machine learning), every end is just a new beginning down another rabbit hole!</em></p>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>🎩 Created by Frank La Vigne | Your Guide Through the Orthogonal Rabbit Hole</p>
            <p style="font-size: 0.9rem; margin-top: 0.5rem; opacity: 0.8;">✨ "It's no use going back to yesterday, because I was a different person then." — But with OSFT, your model remembers who it was!</p>
        </div>
    </footer>

    <script src="app.js"></script>
</body>
</html>
