{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🔄 Orthogonal Fine-Tuning (OFT) Tutorial\n\n## Through the Looking Glass: An Introduction\n\nJust as Alice stepped through the looking glass and found a world that was familiar yet rearranged, Orthogonal Fine-Tuning (OFT) adapts pre-trained models by **rotating** their learned representations rather than distorting them.\n\nOFT is a parameter-efficient fine-tuning (PEFT) technique that applies **orthogonal transformations** to weight matrices—think of it as spinning the looking glass to see your model's knowledge from a new perspective, without warping the reflection. Unlike other PEFT methods like LoRA, OFT preserves the hyperspherical energy – the geometric relationships between neuron activations – ensuring stable and efficient adaptation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. 🔮 The Magic Mirror Properties: Understanding Orthogonal Matrices\n\nAn orthogonal matrix Q is like a magic mirror with special properties. It satisfies: **Q<sup>T</sup>Q = QQ<sup>T</sup> = I**\n\n### Key properties of the looking glass:\n- **Preserves distances:** Alice stays the same size (||Qx|| = ||x||)\n- **Preserves angles:** The Cheshire Cat's grin keeps its shape\n- **Identity when transposed:** The mirror reflects perfectly (Q<sup>T</sup>Q = I)\n- **Represents rotations and reflections:** Turn the mirror, don't bend it\n- **Determinant is ±1:** The mirror's magic constant"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random orthogonal matrix Q:\n",
      "[[-0.60507124  0.67209757  0.38633691  0.18143165]\n",
      " [-0.24292475 -0.64775666  0.7198619   0.05654843]\n",
      " [-0.74974867 -0.30378398 -0.50238365 -0.30529147]\n",
      " [-0.11293046 -0.19081788 -0.28311358  0.9331034 ]]\n",
      "\n",
      "Q^T @ Q (should be identity):\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1. -0.  0.]\n",
      " [ 0. -0.  1. -0.]\n",
      " [ 0.  0. -0.  1.]]\n",
      "\n",
      "Determinant (should be ±1): -0.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import special_ortho_group\n",
    "\n",
    "# Generate a random orthogonal matrix\n",
    "def generate_orthogonal_matrix(dim):\n",
    "    \"\"\"Generate a random orthogonal matrix using QR decomposition\"\"\"\n",
    "    random_matrix = np.random.randn(dim, dim)\n",
    "    q, r = np.linalg.qr(random_matrix)\n",
    "    return q\n",
    "\n",
    "# Verify orthogonality\n",
    "Q = generate_orthogonal_matrix(4)\n",
    "print(\"Random orthogonal matrix Q:\")\n",
    "print(Q)\n",
    "print(\"\\nQ^T @ Q (should be identity):\")\n",
    "print(np.round(Q.T @ Q, 4))\n",
    "print(\"\\nDeterminant (should be ±1):\", np.linalg.det(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. 🪞 Visualizing Orthogonal Transformations\n\nLet's visualize how orthogonal transformations preserve geometric relationships while rotating the feature space.\n\nWatch how the points rotate together, like Wonderland itself spinning—everything moves, but relationships stay true. The mirror spins, but nothing warps!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample 2D data\n",
    "np.random.seed(42)\n",
    "n_points = 100\n",
    "original_data = np.random.randn(n_points, 2)\n",
    "\n",
    "# Create a 2D rotation matrix (orthogonal)\n",
    "theta = np.pi / 4  # 45 degrees\n",
    "rotation_matrix = np.array([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta), np.cos(theta)]\n",
    "])\n",
    "\n",
    "# Apply orthogonal transformation\n",
    "transformed_data = original_data @ rotation_matrix.T\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(original_data[:, 0], original_data[:, 1], alpha=0.6)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlim(-4, 4)\n",
    "axes[0].set_ylim(-4, 4)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Transformed data\n",
    "axes[1].scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.6, color='orange')\n",
    "axes[1].set_title('After Orthogonal Transformation')\n",
    "axes[1].set_xlim(-4, 4)\n",
    "axes[1].set_ylim(-4, 4)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# Overlay comparison\n",
    "axes[2].scatter(original_data[:, 0], original_data[:, 1], alpha=0.6, label='Original')\n",
    "axes[2].scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.6, color='orange', label='Transformed')\n",
    "axes[2].set_title('Comparison: Rotation Preserves Structure')\n",
    "axes[2].set_xlim(-4, 4)\n",
    "axes[2].set_ylim(-4, 4)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_aspect('equal')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify preservation of distances\n",
    "original_distances = np.linalg.norm(original_data[0] - original_data[1])\n",
    "transformed_distances = np.linalg.norm(transformed_data[0] - transformed_data[1])\n",
    "print(f\"Distance between first two points:\")\n",
    "print(f\"  Original: {original_distances:.4f}\")\n",
    "print(f\"  Transformed: {transformed_distances:.4f}\")\n",
    "print(f\"  Preserved: {np.isclose(original_distances, transformed_distances)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. 🎭 The Looking Glass Formula: OFT Core Implementation\n\nThe key insight of OFT is to parameterize weight updates as:\n\n**W' = W × R**\n\nWhere:\n- **W** is the original pre-trained weight matrix (the scene in the mirror)\n- **R** is an orthogonal matrix learned during fine-tuning (the rotation of the looking glass)\n- **W'** is the adapted weight matrix (the new view through the rotated mirror)\n\nThe orthogonal mirror **R** rotates (not reshapes) your model's knowledge, preserving all the geometric relationships that were learned during pre-training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OFTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Orthogonal Fine-Tuning layer implementation.\n",
    "    Applies an orthogonal transformation to the weight matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=16):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = min(rank, min(in_features, out_features))\n",
    "        \n",
    "        # Pre-trained weights (frozen)\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight.requires_grad = False\n",
    "        \n",
    "        # Orthogonal transformation parameters\n",
    "        # We use Cayley parameterization for stable orthogonal updates\n",
    "        self.cayley_map = nn.Parameter(torch.zeros(self.rank, self.rank))\n",
    "        \n",
    "    def get_orthogonal_matrix(self):\n",
    "        \"\"\"\n",
    "        Compute orthogonal matrix using Cayley transform:\n",
    "        R = (I - A)(I + A)^(-1)\n",
    "        where A is skew-symmetric\n",
    "        \"\"\"\n",
    "        # Make skew-symmetric\n",
    "        A = self.cayley_map - self.cayley_map.t()\n",
    "        I = torch.eye(self.rank, device=A.device)\n",
    "        \n",
    "        # Cayley transform\n",
    "        R = torch.linalg.solve(I + A, I - A)\n",
    "        return R\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get orthogonal transformation\n",
    "        R = self.get_orthogonal_matrix()\n",
    "        \n",
    "        # For simplicity, apply R to a subspace of the weight matrix\n",
    "        # In practice, this would be more sophisticated\n",
    "        W_adapted = self.weight.clone()\n",
    "        W_adapted[:self.rank, :self.rank] = self.weight[:self.rank, :self.rank] @ R\n",
    "        \n",
    "        return x @ W_adapted.t()\n",
    "\n",
    "# Example usage\n",
    "oft_layer = OFTLayer(128, 64, rank=16)\n",
    "input_tensor = torch.randn(32, 128)  # batch_size=32, in_features=128\n",
    "output = oft_layer(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in oft_layer.parameters() if p.requires_grad)}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in oft_layer.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. 📊 Comparing OFT with LoRA\n\nLet's implement a simple comparison between OFT and LoRA to understand their differences.\n\nWhile both methods achieve parameter efficiency, they work in fundamentally different ways:\n- **OFT** rotates the feature space (multiplicative, preserves geometry)\n- **LoRA** adds low-rank updates (additive, more flexible but can distort relationships)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-Rank Adaptation (LoRA) layer for comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=16):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        \n",
    "        # Pre-trained weights (frozen)\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight.requires_grad = False\n",
    "        \n",
    "        # LoRA parameters\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original transformation + low-rank update\n",
    "        return x @ self.weight.t() + x @ self.lora_A.t() @ self.lora_B.t()\n",
    "\n",
    "# Compare parameter efficiency\n",
    "in_features, out_features = 512, 256\n",
    "rank = 16\n",
    "\n",
    "oft_layer = OFTLayer(in_features, out_features, rank)\n",
    "lora_layer = LoRALayer(in_features, out_features, rank)\n",
    "\n",
    "oft_params = sum(p.numel() for p in oft_layer.parameters() if p.requires_grad)\n",
    "lora_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "total_params = in_features * out_features\n",
    "\n",
    "print(\"Parameter Efficiency Comparison:\")\n",
    "print(f\"Original layer parameters: {total_params:,}\")\n",
    "print(f\"OFT trainable parameters: {oft_params:,} ({oft_params/total_params*100:.2f}%)\")\n",
    "print(f\"LoRA trainable parameters: {lora_params:,} ({lora_params/total_params*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. 🎩 The Mad Hatter's Tea Party: Hyperspherical Energy Preservation\n\nA key advantage of OFT is preserving **hyperspherical energy**, which maintains the angular relationships between features.\n\n### The Mad Hatter's Tea Party Problem\n\nImagine you're at the Mad Hatter's tea party, and everyone's seat represents a learned feature in your model:\n\n- **Traditional fine-tuning** is like the Hatter shouting \"Move down!\" - everyone shifts chaotically, and suddenly the March Hare is sitting where the Dormouse should be. The entire seating arrangement (your model's learned relationships) gets scrambled.\n\n- **OFT is different.** It's like rotating the entire table instead of moving individual seats. Everyone maintains their relative positions—if Alice was between the Hatter and the Hare before, she still is after the rotation. The *relationships* stay intact.\n\nBy preserving angular relationships (hyperspherical energy), OFT maintains the semantic structure learned during pre-training. Features that were similar before fine-tuning remain similar after. This leads to more stable training and better generalization!\n\n💫 *Down here in Wonderland, we don't break what already works - we just rotate it to see it from a new angle.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hyperspherical_energy(features):\n",
    "    \"\"\"\n",
    "    Compute hyperspherical energy as the sum of pairwise angular similarities.\n",
    "    Lower energy indicates more uniform distribution on the hypersphere.\n",
    "    \"\"\"\n",
    "    # Normalize features to unit sphere\n",
    "    normalized = features / (torch.norm(features, dim=-1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    # Compute pairwise cosine similarities\n",
    "    similarities = torch.matmul(normalized, normalized.t())\n",
    "    \n",
    "    # Hyperspherical energy (Riesz s-energy with s=1)\n",
    "    n = features.shape[0]\n",
    "    mask = 1 - torch.eye(n)\n",
    "    energy = -torch.sum(torch.log(1 - similarities + 1e-8) * mask) / (n * (n - 1))\n",
    "    \n",
    "    return energy\n",
    "\n",
    "# Generate sample features\n",
    "torch.manual_seed(42)\n",
    "n_samples = 50\n",
    "n_features = 64\n",
    "\n",
    "# Original features\n",
    "original_features = torch.randn(n_samples, n_features)\n",
    "\n",
    "# Apply different transformations\n",
    "# 1. Orthogonal transformation (OFT-style)\n",
    "Q = torch.tensor(generate_orthogonal_matrix(n_features), dtype=torch.float32)\n",
    "oft_features = original_features @ Q.t()\n",
    "\n",
    "# 2. Random transformation (non-orthogonal)\n",
    "random_matrix = torch.randn(n_features, n_features) * 0.5 + torch.eye(n_features)\n",
    "random_features = original_features @ random_matrix.t()\n",
    "\n",
    "# 3. Low-rank update (LoRA-style)\n",
    "A = torch.randn(16, n_features) * 0.1\n",
    "B = torch.randn(n_features, 16) * 0.1\n",
    "lora_features = original_features + original_features @ A.t() @ B.t()\n",
    "\n",
    "# Compute energies\n",
    "original_energy = compute_hyperspherical_energy(original_features)\n",
    "oft_energy = compute_hyperspherical_energy(oft_features)\n",
    "random_energy = compute_hyperspherical_energy(random_features)\n",
    "lora_energy = compute_hyperspherical_energy(lora_features)\n",
    "\n",
    "print(\"Hyperspherical Energy Comparison:\")\n",
    "print(f\"Original features: {original_energy:.4f}\")\n",
    "print(f\"After OFT (orthogonal): {oft_energy:.4f} (change: {abs(oft_energy - original_energy):.4f})\")\n",
    "print(f\"After random transform: {random_energy:.4f} (change: {abs(random_energy - original_energy):.4f})\")\n",
    "print(f\"After LoRA update: {lora_energy:.4f} (change: {abs(lora_energy - original_energy):.4f})\")\n",
    "print(\"\\nOFT preserves hyperspherical energy best!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. ⚙️ Practical OFT Module for Transformer Models\n\nLet's implement a more practical OFT module that can be integrated into transformer-based models.\n\nThis production-ready module uses a **block-diagonal structure** for efficiency—like having multiple smaller looking glasses instead of one giant mirror. Each block can rotate independently, making the computation more tractable while maintaining the orthogonal properties."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OFTModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Production-ready OFT module with block-diagonal structure for efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_module, rank=16, num_blocks=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.original_module = original_module\n",
    "        self.rank = rank\n",
    "        self.num_blocks = num_blocks\n",
    "        self.alpha = alpha  # scaling for the adaptation\n",
    "\n",
    "        if not isinstance(original_module, nn.Linear):\n",
    "            raise ValueError(\"OFT currently supports only Linear layers\")\n",
    "        self.in_features = original_module.in_features\n",
    "        self.out_features = original_module.out_features\n",
    "\n",
    "        # Choose block size\n",
    "        self.block_size = min(rank, min(self.in_features, self.out_features) // num_blocks)\n",
    "\n",
    "        # Trainable skew-symmetric seeds for each block (square)\n",
    "        self.blocks = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(self.block_size, self.block_size))\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # Freeze base weights/bias\n",
    "        for p in self.original_module.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def get_block_diagonal_orthogonal(self):\n",
    "        \"\"\"\n",
    "        Differentiable block-diagonal orthogonal matrix via Cayley transform.\n",
    "        \"\"\"\n",
    "        device = self.blocks[0].device\n",
    "        I = torch.eye(self.block_size, device=device)\n",
    "\n",
    "        per_block_R = []\n",
    "        for B in self.blocks:\n",
    "            A = B - B.t()                       # skew-symmetric\n",
    "            # (I - A)(I + A)^{-1} is also common; either is fine if consistent.\n",
    "            block_R = torch.linalg.solve(I + A, I - A)\n",
    "            per_block_R.append(block_R)\n",
    "\n",
    "        # Differentiable assembly (no in-place writes)\n",
    "        R = torch.block_diag(*per_block_R)      # shape: (k, k)\n",
    "        return R\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original output (constant w.r.t. OFT params)\n",
    "        original_output = self.original_module(x)\n",
    "\n",
    "        # Build rotation\n",
    "        R = self.get_block_diagonal_orthogonal()\n",
    "        k = R.shape[0]\n",
    "\n",
    "        # Base weight (constant path); DO NOT detach here—let autograd trace through the parts that depend on R\n",
    "        W_base = self.original_module.weight    # [out_features, in_features]\n",
    "\n",
    "        # Top-left block adapted (differentiable wrt R)\n",
    "        W_tl = W_base[:k, :k]                   # view (read-only)\n",
    "        W_tl_adapted = W_tl @ R                 # depends on R (grad flows)\n",
    "\n",
    "        # Reconstruct full adapted weight without in-place writes or aliasing\n",
    "        # Top row: [TL_adapted | TR_const]\n",
    "        top_right = W_base[:k, k:]\n",
    "        top = torch.cat([W_tl_adapted, top_right], dim=1)\n",
    "\n",
    "        # Bottom rows unchanged\n",
    "        bottom = W_base[k:, :]\n",
    "\n",
    "        adapted_weight = torch.cat([top, bottom], dim=0)\n",
    "\n",
    "        # Compute adapted output with differentiable weight\n",
    "        adapted_output = nn.functional.linear(x, adapted_weight, self.original_module.bias)\n",
    "\n",
    "        # Blend\n",
    "        return (1 - self.alpha) * original_output + self.alpha * adapted_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. 🧪 Training Example with OFT\n\nLet's demonstrate how to train a model using OFT for a simple classification task.\n\nIn this example, we'll create a simple classifier and compare a regular model with an OFT-adapted version. Notice how OFT achieves dramatic parameter reduction (often 90%+) while maintaining the model's learning capacity!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Generate synthetic classification data\n",
    "torch.manual_seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 64\n",
    "n_classes = 10\n",
    "\n",
    "X = torch.randn(n_samples, n_features)\n",
    "y = torch.randint(0, n_classes, (n_samples,))\n",
    "\n",
    "# Split into train and test\n",
    "train_size = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define a simple model with OFT adaptation\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, use_oft=False):\n",
    "        super().__init__()\n",
    "        self.use_oft = use_oft\n",
    "        \n",
    "        # Create layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Apply OFT if requested\n",
    "        if use_oft:\n",
    "            self.fc1 = OFTModule(self.fc1, rank=8, num_blocks=2)\n",
    "            self.fc2 = OFTModule(self.fc2, rank=8, num_blocks=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create two models: one with OFT, one without\n",
    "model_oft = SimpleClassifier(n_features, 128, n_classes, use_oft=True)\n",
    "model_regular = SimpleClassifier(n_features, 128, n_classes, use_oft=False)\n",
    "\n",
    "# Count parameters\n",
    "oft_trainable = sum(p.numel() for p in model_oft.parameters() if p.requires_grad)\n",
    "regular_trainable = sum(p.numel() for p in model_regular.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"Regular model trainable parameters: {regular_trainable:,}\")\n",
    "print(f\"OFT model trainable parameters: {oft_trainable:,}\")\n",
    "print(f\"Parameter reduction: {(1 - oft_trainable/regular_trainable)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, epochs=10, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_losses[-1]:.4f}, Test Acc: {accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "# Train OFT model\n",
    "print(\"Training OFT Model:\")\n",
    "oft_losses, oft_accs = train_model(model_oft, train_loader, test_loader, epochs=10)\n",
    "\n",
    "# For comparison, we could also train the regular model\n",
    "# Note: In practice, OFT is used for fine-tuning pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. 🌹 The Queen's Decree: Advantages and Use Cases of OFT\n\n### Key Advantages\n\n1. **Stability** 👑 - \"All ways are MY ways!\" Orthogonal constraints keep gradients well-behaved—no explosion, no vanishing. The Queen's rules (mathematical constraints) ensure orderly training.\n\n2. **Parameter Efficiency** 🎩 - \"Why is a raven like a writing desk?\" Because both OFT achieves 90%+ parameter reduction! Fewer parameters to tune, but the tea party keeps its charm.\n\n3. **Geometric Preservation** 🪞 - The looking glass principle: rotations preserve the learned feature relationships, maintaining the model's core understanding.\n\n4. **Better Generalization** 💫 - Less prone to overfitting on small datasets because we preserve the robust representations learned during pre-training.\n\n### Best Use Cases\n\n- **Domain Adaptation** 🎯 - When fine-tuning to a related but different domain without losing general knowledge\n- **Few-Shot Learning** 🔬 - When training data is limited and you need to maintain robustness\n- **Multi-Task Learning** 📚 - Preserving shared representations across tasks\n- **Continual Learning** 🤖 - Reducing catastrophic forgetting (more on this with OSFT!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Stability comparison\n",
    "def analyze_gradient_flow(model, input_data, target):\n",
    "    \"\"\"\n",
    "    Analyze gradient magnitudes through the network.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradient norms\n",
    "    gradient_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            gradient_norms.append(grad_norm)\n",
    "            print(f\"{name}: gradient norm = {grad_norm:.6f}\")\n",
    "    \n",
    "    return gradient_norms\n",
    "\n",
    "# Create test data\n",
    "test_input = torch.randn(32, n_features)\n",
    "test_target = torch.randint(0, n_classes, (32,))\n",
    "\n",
    "print(\"Gradient Flow Analysis for OFT Model:\")\n",
    "print(\"=\"*50)\n",
    "grad_norms = analyze_gradient_flow(model_oft, test_input, test_target)\n",
    "print(f\"\\nAverage gradient norm: {np.mean(grad_norms):.6f}\")\n",
    "print(f\"Gradient variance: {np.var(grad_norms):.6f}\")\n",
    "print(\"\\nNote: OFT maintains stable gradients due to orthogonal constraints!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🎪 Summary: Lessons from Wonderland\n\nOrthogonal Fine-Tuning (OFT) represents a significant advancement in parameter-efficient fine-tuning:\n\n### 🪞 The Looking Glass Principle\nLike spinning a mirror rather than cracking it, orthogonal transformations rotate the feature space without distortion. Alice stays Alice, just viewed from a new angle—geometric relationships preserved perfectly.\n\n### 🎩 The Mad Hatter's Efficiency  \nAchieve 90%+ parameter reduction while maintaining model capacity! More efficient than full fine-tuning, just as effective. The orthogonal constraint ensures that fine-tuning rotates the feature space rather than arbitrarily distorting it.\n\n### 🌹 The Queen's Stability\nOrthogonal constraints ensure orderly, stable training compared to unconstrained methods. Off with gradient chaos!\n\n### 💫 Better for What Matters\nOFT is particularly well-suited for tasks requiring preservation of learned features—domain adaptation, few-shot learning, and scenarios where you can't afford to forget what the model already knows.\n\n🐇 *\"Curiouser and curiouser!\" cried Alice. And indeed—the deeper you go down this orthogonal rabbit hole, the more elegant the mathematics becomes.*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}